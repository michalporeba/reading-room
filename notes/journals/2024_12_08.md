- Reading [[Modern Data Engineering with Apache Spark]]
	- The author, [[Scott Haines]] is a software engineer interested in distribute and applied data systems. he drove adoption of [[Apache Spark]] at [[Twilio]] for [[Stream Processing]].
	- sample code is at https://github.com/newfront/spark-moderndataengineering
	- For context, the book starts with history of doing software engineering from 1990s to modern time, from pre-cloud, including hosted services with companies like [[Rackspace]], [[GoDaddy]] and [[Media Temple]]. Then going into the cloud, and finally the public cloud.
	- [[Quote]]: As a specialization, data engineering emerged from the roots of traditional *software engineering*, *data analytics*, and *machine learning* due to an ever-increasing demand for data and the evolving data needs by companies and organizations over time. The need to bridge these disciplines led to a new engineering specialization focusing on data quality, availability, accessibility, governance, maintainability, and observability at scale.
	- [[Quote]]: Data engineering today is essentially all about controlling the flow of data trough services and between different databases, data streams, data lakes, and warehouses. This flow of data through these data networks supports different access and processing styles that can be applied to the captured data, ranging from historic data analysis, insight generation, model training for machine learning and intelligence, as well as fast access to data for metric serving, operational insights and monitoring, and for real-time model serving and intelligence.
	- [[Quote]]: To ve successful and meet the current data demands, data engineers must understand how to reliably capture, process, and scale the torrents of data that are generated across the entire data platform, and within the full data ecosystem at large. This requires a solid understanding of software of software engineering best practices, distributed systems, a concrete understanding of SQL (including data modeling, querying, and query optimization), as well as knowledge pertaining to analytical processing and querying, and even the basics of statistics and machine learning.
	- LATER [[Spark: Cluster Computing with Working Sets]] - 2010 paper by Zaharia et al.
	- LATER [[Map Reduce: Simplified Data Processing on Large Clusters]] - 2004 paper by [[Jeffrey Dean]] and [[Sanjay Ghemawat]] from [[Google]]
	- [[Apache Spark]] depends on [[Resilient Distributed Data Model]] (RDD) which is "a read-only, immutable collection of data partitioned across a set of network-connected servers that are bount to a Spark application".
	-